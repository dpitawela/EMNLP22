{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated = ['entropy interprets the degree of maximum coverage']\n",
    "generated = ['The core idea The core idea The core idea The core idea The core idea The core idea']\n",
    "# golden = ['The core idea The core idea The core idea The core idea The core idea The core idea']\n",
    "golden = ['The By definition entropy encompasses the notion of maximum coverage.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for summary in generated:\n",
    "def calculateRedundancy(summary, n_gram):\n",
    "    semantic_unit_counts = {}\n",
    "    grams = nltk.ngrams(summary.split(), n_gram) # splitting to ngrams\n",
    "    \n",
    "    # counting the frequency of semantic units\n",
    "    for gram in grams:\n",
    "        if gram in semantic_unit_counts:\n",
    "            semantic_unit_counts[gram] += 1\n",
    "        else:\n",
    "            semantic_unit_counts[gram] = 1\n",
    "\n",
    "    # getting the total number of semantic units\n",
    "    n_semantic_units = semantic_unit_counts.keys().__len__()\n",
    "\n",
    "    entropy = 0 # initialising entropy for current summary\n",
    "    for unit, count in semantic_unit_counts.items():\n",
    "        prob = (count / n_semantic_units)\n",
    "        entropy += -(prob * math.log(prob, 2))\n",
    "\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "n_gram = 1 # set the semantic unit length\n",
    "redundancy = 0 # initialising redundancy for the whole test set\n",
    "\n",
    "for summary in generated:\n",
    "    # adding entropy calculated for a single example into the total redundancy\n",
    "    redundancy += calculateRedundancy(summary=summary, n_gram=n_gram)\n",
    "    \n",
    "print(redundancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRelevance(generatedSummary, goldenSummary, n_gram):\n",
    "    semantic_unit_counts_generated = {}\n",
    "    semantic_unit_counts_golden = {}\n",
    "    \n",
    "    grams_generated = nltk.ngrams(generatedSummary.split(), n_gram) # splitting to ngrams\n",
    "    grams_golden = nltk.ngrams(goldenSummary.split(), n_gram) # splitting to ngrams\n",
    "    \n",
    "    # counting the frequency of semantic units in generated\n",
    "    for gram in grams_generated:\n",
    "        if gram in semantic_unit_counts_generated:\n",
    "            semantic_unit_counts_generated[gram] += 1\n",
    "        else:\n",
    "            semantic_unit_counts_generated[gram] = 1\n",
    "    \n",
    "    # counting the frequency of semantic units in generated\n",
    "    for gram in grams_golden:\n",
    "        if gram in semantic_unit_counts_golden:\n",
    "            semantic_unit_counts_golden[gram] += 1\n",
    "        else:\n",
    "            semantic_unit_counts_golden[gram] = 1\n",
    "\n",
    "    # getting the total number of semantic units\n",
    "    n_semantic_units_generated = semantic_unit_counts_generated.keys().__len__()\n",
    "    n_semantic_units_golden = semantic_unit_counts_golden.keys().__len__()\n",
    "\n",
    "    relevance = 0 # initialising relevance for the current summary\n",
    "    for (unit_gen, count_gen) in semantic_unit_counts_generated.items():\n",
    "        \n",
    "        prob_gen = (count_gen / n_semantic_units_generated)\n",
    "        prob_gold = ((semantic_unit_counts_golden[unit_gen] if unit_gen in semantic_unit_counts_golden else 0) / n_semantic_units_golden)\n",
    "        # print(unit_gen, prob_gen, prob_gold)\n",
    "\n",
    "        if prob_gen != 0 and prob_gold != 0:\n",
    "            relevance += (prob_gen * math.log(prob_gold, 2))\n",
    "        else:\n",
    "            relevance += 0\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.643856189774724"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram = 1 # set the semantic unit length\n",
    "relevance = 0 # initialising relevance for the whole test set\n",
    "for generatedSummary, goldenSummary in zip(generated, golden):\n",
    "    # adding relevance calculated for a single example into the total relevance\n",
    "    relevance += calculateRelevance(generatedSummary=generatedSummary, goldenSummary=goldenSummary, n_gram=n_gram)\n",
    "\n",
    "relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d766252c1fdaba7e6ce3305c0a5bc918554b111c33d5473fab85a34370b8e586"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('evaltoolkit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
